{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Downloads'\n",
      "/Users/dlaldea/Downloads\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd Downloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solve the ridge regression problem on the training set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "def Read_Data_SVM(filename):\n",
    "    data=load_svmlight_file(filename)\n",
    "    X=data[0]\n",
    "    y=data[1].reshape(-1,1)\n",
    "    return X,y\n",
    "\n",
    "X,y=Read_Data_SVM('cpusmall_scale.txt')\n",
    "y=preprocessing.maxabs_scale(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Split_Data(y,X, test_ratio):\n",
    "    n=X.shape\n",
    "    indices=np.random.permutation(n[0])\n",
    "    test_set_size=int(n[0]*test_ratio)\n",
    "    test_indices=indices[:test_set_size]\n",
    "    train_indices=indices[test_set_size:]\n",
    "    return X[train_indices,:], X[test_indices,:], y[train_indices], y[test_indices]\n",
    "\n",
    "training, testing, y_train, y_test=Split_Data(y,X,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mse for lambda=', 1, 'is:', 0.0092)\n"
     ]
    }
   ],
   "source": [
    "#Ridge Regression on Training Set\n",
    "def Ridge_Reg(training, testing, y_test, y_train, L):\n",
    "    n = np.shape(training)\n",
    "    theta_hat=np.random.rand(n[1],1)#creates a p-row*1-col\n",
    "    w=np.linalg.inv(np.transpose(training).dot(training)+L*np.eye(n[1])).dot(np.transpose(training).dot(y_train))\n",
    "    #Compute MSE on testing set\n",
    "    m=np.shape(testing)\n",
    "    test_pred=testing.dot(w)\n",
    "    mse=mean_squared_error(test_pred,y_test)\n",
    "    \n",
    "    print(\"mse for lambda=\",L, \"is:\",round(mse,4))\n",
    "\n",
    "Ridge_Reg(training, testing, y_test, y_train, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Ridge Regression for multiple lambdas and report the test MSE for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mse for lambda=', 0.01, 'is:', 0.0092)\n",
      "('mse for lambda=', 0.1, 'is:', 0.0092)\n",
      "('mse for lambda=', 1, 'is:', 0.0092)\n",
      "('mse for lambda=', 10, 'is:', 0.0094)\n",
      "('mse for lambda=', 100, 'is:', 0.011)\n"
     ]
    }
   ],
   "source": [
    "L_vec=[0.01,0.1,1,10,100]\n",
    "\n",
    "for L in L_vec:\n",
    "    Ridge_Reg(training, testing, y_test, y_train, L)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Ridge Regression with Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0092\n",
      "('MSE for lambda=', 0.01, 'is', 0.0092)\n",
      "('MSE for lambda=', 0.1, 'is', 0.0092)\n",
      "('MSE for lambda=', 1, 'is', 0.0092)\n",
      "('MSE for lambda=', 10, 'is', 0.0094)\n",
      "('MSE for lambda=', 100, 'is', 0.011)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg=Ridge(alpha=1, solver=\"cholesky\",fit_intercept=False)\n",
    "ridge_reg.fit(training, y_train)\n",
    "yr=ridge_reg.predict(testing)\n",
    "print round(sklearn.metrics.mean_squared_error(y_test,yr),4) \n",
    "\n",
    "#Problem 1.1 and 1.2 Try for multiple lambdas:\n",
    "L_vec=[0.01,0.1,1,10,100]\n",
    "for L in L_vec:\n",
    "    ridge_reg=Ridge(alpha=L, solver=\"cholesky\", fit_intercept=False)\n",
    "    ridge_reg.fit(training, y_train)\n",
    "    yr=ridge_reg.predict(testing)\n",
    "    mse=round(sklearn.metrics.mean_squared_error(y_test,yr),4)\n",
    "    print (\"MSE for lambda=\",L,\"is\",mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 1.3 & 1.4 given a stepsize report MSE with E2006 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "train_E2006, yE2006_train=Read_Data_SVM(\"E2006.train\")\n",
    "test_E2006, yE2006_test=Read_Data_SVM(\"E2006.test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_E2006=train_E2006[:,0:150358]\n",
    "train_scaled=preprocessing.maxabs_scale(train_E2006)\n",
    "test_scaled=preprocessing.maxabs_scale(test_E2006)\n",
    "\n",
    "ytrain_scaled=preprocessing.maxabs_scale(yE2006_train)\n",
    "ytest_scaled=preprocessing.maxabs_scale(yE2006_test)\n",
    "\n",
    "n=train_scaled.shape\n",
    "theta_hat=np.random.rand(n[1],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient(x_t,x,theta,y,L):\n",
    "    y_pred=x.dot(theta)\n",
    "    g=x_t.dot(y_pred-y)+L*theta\n",
    "    return g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def StepWise_G(w,train, y, step, e):\n",
    "    L=1\n",
    "    train_t=np.transpose(train)\n",
    "    r0=np.linalg.norm(gradient(train_t,train,w,y,L))\n",
    "    for i in range(0,50):\n",
    "        g=gradient(train_t,train,w,y,L)\n",
    "        g_norm=np.linalg.norm(g)\n",
    "        if g_norm <= e*r0:\n",
    "            print(g_norm)\n",
    "            break\n",
    "        w=w-step*g\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE_testing for step=', 1e-07, 'is', 3577.604)\n",
      "('MSE_testing for step=', 1e-06, 'is', 506.091)\n",
      "('MSE_testing for step=', 1e-05, 'is', 296.667)\n",
      "('MSE_testing for step=', 0.0001, 'is', 8.383984832197233e+99)\n",
      "('MSE_testing for step=', 0.001, 'is', 1.0062698705286456e+204)\n",
      "('MSE_testing for step=', 0.01, 'is', 2.531266086469879e+304)\n"
     ]
    }
   ],
   "source": [
    "step_vec=[1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "m=test_scaled.shape\n",
    "for step in step_vec:\n",
    "    theta_hat=np.random.rand(n[1],1)\n",
    "    w=StepWise_G(theta_hat,train_scaled,ytrain_scaled,step,0.001)\n",
    "    y_pred=test_scaled.dot(w)\n",
    "    mse=round(mean_squared_error(ytest_scaled,y_pred),3)\n",
    "    #accuracy=accuracy_score(ytest_scaled, y_pred)\n",
    "    print(\"MSE_testing for step=\", step, \"is\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE_testing for step=', 1e-07, 'is', 1.135)\n",
      "('MSE_testing for step=', 1e-06, 'is', 0.166)\n",
      "('MSE_testing for step=', 1e-05, 'is', 0.163)\n",
      "('MSE_testing for step=', 0.0001, 'is', 2.401810359311467e+126)\n",
      "('MSE_testing for step=', 0.001, 'is', 3.3555075473468818e+227)\n",
      "('MSE_testing for step=', 0.01, 'is', inf)\n"
     ]
    }
   ],
   "source": [
    "#try with non-scaled data\n",
    "step_vec=[1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "m=test_E2006.shape\n",
    "for step in step_vec:\n",
    "    theta_hat=np.random.rand(n[1],1)\n",
    "    w=StepWise_G(theta_hat,train_E2006,yE2006_train,step,0.001)\n",
    "    y_pred=test_E2006.dot(w)\n",
    "    mse=round(mean_squared_error(yE2006_test,y_pred),3)\n",
    "    #accuracy=accuracy_score(ytest_scaled, y_pred)\n",
    "    print(\"MSE_testing for step=\", step, \"is\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Ridge Gradient Descent for E2006 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE for learning_rate=', 1e-07, 'is:', 0.2791)\n",
      "('MSE for learning_rate=', 1e-06, 'is:', 0.1902)\n",
      "('MSE for learning_rate=', 1e-05, 'is:', 0.019)\n",
      "('MSE for learning_rate=', 0.0001, 'is:', 0.0115)\n",
      "('MSE for learning_rate=', 0.001, 'is:', 0.0115)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor \n",
    "#Problem 1.3 and 1.4 with scaled data\n",
    "step_vec=[1e-7,1e-6, 1e-5, 1e-4,1e-3]\n",
    "for step in step_vec:\n",
    "#Using Stochastic Gradient Descent\n",
    "    sgd_reg=SGDRegressor(penalty=\"l2\", alpha=1,fit_intercept=False,n_iter=50, eta0=step)\n",
    "    sgd_reg.fit(train_scaled, ytrain_scaled.ravel())\n",
    "    yr_SGD=sgd_reg.predict(test_scaled)\n",
    "    mse=round(mean_squared_error(ytest_scaled,yr_SGD),4)\n",
    "    print(\"MSE for learning_rate=\",step,\"is:\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE for learning_rate=', 1e-07, 'is:', 13.6581)\n",
      "('MSE for learning_rate=', 1e-06, 'is:', 6.4144)\n",
      "('MSE for learning_rate=', 1e-05, 'is:', 0.2118)\n",
      "('MSE for learning_rate=', 0.0001, 'is:', 0.1944)\n",
      "('MSE for learning_rate=', 0.001, 'is:', 0.193)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor \n",
    "#Problem 1.3 and 1.4 with non-scaled data\n",
    "step_vec=[1e-7,1e-6, 1e-5, 1e-4,1e-3]\n",
    "for step in step_vec:\n",
    "#Using Stochastic Gradient Descent\n",
    "    sgd_reg=SGDRegressor(penalty=\"l2\", alpha=1,fit_intercept=True,n_iter=50, eta0=step)\n",
    "    sgd_reg.fit(train_E2006, yE2006_train.ravel())\n",
    "    yr_SGD=sgd_reg.predict(test_E2006)\n",
    "    mse=round(mean_squared_error(yE2006_test,yr_SGD),4)\n",
    "    print(\"MSE for learning_rate=\",step,\"is:\",mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Classification (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Downloads'\n",
      "/Users/dlaldea/Downloads\n"
     ]
    }
   ],
   "source": [
    "%cd Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Read_Data_SVM(filename):\n",
    "    data=load_svmlight_file(filename)\n",
    "    X=data[0]\n",
    "    y=data[1].reshape(-1,1)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Split_Data(y,X, test_ratio):\n",
    "    n=X.shape\n",
    "    indices=np.random.permutation(n[0])\n",
    "    test_set_size=int(n[0]*test_ratio)\n",
    "    test_indices=indices[:test_set_size]\n",
    "    train_indices=indices[test_set_size:]\n",
    "    return X[train_indices,:], X[test_indices,:], y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Problem 2.2\n",
    "news_X, news_y=Read_Data_SVM('news20.binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = Split_Data(news_y, news_X,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train=preprocessing.maxabs_scale(X_train)\n",
    "X_test=preprocessing.maxabs_scale(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=X_train.shape\n",
    "theta=np.random.rand(n[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Gradient_Log(y,w,xty, L):\n",
    "    e=np.exp(w.T.dot(xty))\n",
    "    g=(-xty/(1+e))-L*w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD_Log(w,train, y, step, e):\n",
    "    L=1\n",
    "    xty=train.T.dot(y)\n",
    "    r0=np.linalg.norm(Gradient_Log(y,w,xty,L))\n",
    "    for i in range(0,50):\n",
    "        g=Gradient_Log(y,w,xty,L)\n",
    "        g_norm=np.linalg.norm(g)\n",
    "        if g_norm <= e*r0:\n",
    "            return w\n",
    "            break\n",
    "        w=w-step*g\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy for step=', 1e-07, 'is', 0.5004)\n",
      "('Accuracy for step=', 1e-06, 'is', 0.5004)\n",
      "('Accuracy for step=', 1e-05, 'is', 0.5004)\n",
      "('Accuracy for step=', 0.0001, 'is', 0.5004)\n",
      "('Accuracy for step=', 0.001, 'is', 0.5024)\n",
      "('Accuracy for step=', 0.01, 'is', 0.5076)\n",
      "('Accuracy for step=', 0.1, 'is', 0.8597)\n",
      "('Accuracy for step=', 1, 'is', 0.804)\n"
     ]
    }
   ],
   "source": [
    "step_vec=[1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1]\n",
    "for step in step_vec:\n",
    "    w=SGD_Log(theta,X_train,y_train,step,0.001)\n",
    "    w_pred=X_test.dot(w)\n",
    "    labels= map(lambda label: -1 if np.sign(label)== -1 else 1, w_pred)\n",
    "    accuracy=round(accuracy_score(y_test, labels),4)\n",
    "    print(\"Accuracy for step=\", step, \"is\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: SGD for Logistic using Scikit Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('accuracy for lambda=1 and step=', 1e-06, 'is', 0.8132)\n",
      "('accuracy for lambda=1 and step=', 1e-05, 'is', 0.8132)\n",
      "('accuracy for lambda=1 and step=', 0.0001, 'is', 0.8132)\n",
      "('accuracy for lambda=1 and step=', 0.001, 'is', 0.8132)\n",
      "('accuracy for lambda=1 and step=', 0.01, 'is', 0.8132)\n",
      "('accuracy for lambda=1 and step=', 0.1, 'is', 0.8132)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "step_vec=[1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "for step in step_vec:\n",
    "#with fixed step-size for alpha=1 and different step\n",
    "    clf=SGDClassifier(loss=\"log\", penalty=\"l2\", alpha=1,eta0=step,n_iter=50, fit_intercept=False)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    yr=clf.predict(X_test)\n",
    "    accuracy=round(accuracy_score(y_test,yr),4)\n",
    "    print(\"accuracy for lambda=1 and step=\",step,\"is\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.3 Line Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(w,L,xty):\n",
    "    f=(np.log(1+np.exp(-w.T.dot(xty)))+(L/2)*np.linalg.norm(w)**2)\n",
    "    return f\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gradient Descent with line search \n",
    "\n",
    "def LineGDesc(x,y,w,e,L):\n",
    "    xty=x.T.dot(y) #p*1\n",
    "    r0=np.linalg.norm(Gradient_Log(y,w,xty,L))\n",
    "    for i in range(0,50):\n",
    "        g=Gradient_Log(y,w,xty,L)\n",
    "        g_norm=np.linalg.norm(g)\n",
    "        step=1\n",
    "        if g_norm <=e*r0:\n",
    "            break\n",
    "        else: \n",
    "        #We define the fist instances\n",
    "            b=True\n",
    "            while b:\n",
    "                #We update for each step wht new_f is \n",
    "                w_test=w-step*g\n",
    "                if f(w_test,L,xty)>= f(w, L, xty):\n",
    "                    step=step/2.0\n",
    "                else: \n",
    "                    b=False\n",
    "                if step <= 1e-5:\n",
    "                    break\n",
    "            w=w_test-step*g\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy for lambda=', 1e-06, 'is', 0.8005)\n",
      "('Accuracy for lambda=', 1e-05, 'is', 0.8005)\n",
      "('Accuracy for lambda=', 0.0001, 'is', 0.8005)\n",
      "('Accuracy for lambda=', 0.001, 'is', 0.8005)\n",
      "('Accuracy for lambda=', 0.01, 'is', 0.8007)\n",
      "('Accuracy for lambda=', 0.1, 'is', 0.8037)\n"
     ]
    }
   ],
   "source": [
    "L=[1e-6, 1e-5, 1e-4, 1e-3, 1e-2,1e-1]\n",
    "theta=np.random.rand(n[1],1)\n",
    "for l in L:\n",
    "    w=LineGDesc(X_train,y_train,theta,0.001,l)\n",
    "    w_pred=X_test.dot(w)\n",
    "    labels=map(lambda label: -1 if np.sign(label) == -1 else 1, w_pred)\n",
    "    accuracy=round(accuracy_score(y_test, labels),4)\n",
    "    print(\"Accuracy for lambda=\",l , \"is\",accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
