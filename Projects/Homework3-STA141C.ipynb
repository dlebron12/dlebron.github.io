{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solve the ridge regression problem on the training set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Downloads'\n",
      "/Users/dlaldea/Downloads\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd Downloads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Read_Data(filename):\n",
    "    y=[]\n",
    "    rows=[]\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.split()\n",
    "            y.append([float(line[0])])\n",
    "            rows.append([float(x.split(':')[1]) for x in line[1:]])\n",
    "    y=np.array(y)\n",
    "    X=np.matrix(rows)\n",
    "    return X,y\n",
    "\n",
    "X,y=Read_Data('cpusmall_scale.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Read_Data_SVM(filename):\n",
    "    data=load_svmlight_file(filename)\n",
    "    X=data[0]\n",
    "    y=data[1].reshape(-1,1)\n",
    "    return X,y\n",
    "\n",
    "#X,y=Read_Data_SVM('cpusmall_scale.txt')\n",
    "#X=X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_abs_scaler=preprocessing.MaxAbsScaler()\n",
    "y=max_abs_scaler.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Split_Data(y,X, test_ratio):\n",
    "    np.random.seed(42)\n",
    "    n=len(X)\n",
    "    indices=np.random.permutation(n)\n",
    "    test_set_size=int(n*test_ratio)\n",
    "    test_indices=indices[:test_set_size]\n",
    "    train_indices=indices[test_set_size:]\n",
    "    return X[train_indices,:], X[test_indices,:], y[train_indices], y[test_indices]\n",
    "\n",
    "#training, testing, y_train, y_test=Split_Data(y,X,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mse for lambda=', 100, 'is:', matrix([[ 0.01097495]]))\n"
     ]
    }
   ],
   "source": [
    "#Ridge Regression on Training Set\n",
    "\n",
    "def Ridge_Reg(training, testing, y_test, y_train, L):\n",
    "    n = np.shape(training)\n",
    "    theta_hat=np.random.rand(n[1],1)#creates a p-row*1-col\n",
    "    w=np.linalg.inv(np.transpose(training).dot(training)+L*np.eye(n[1])).dot(np.transpose(training).dot(y_train))\n",
    "    #Compute MSE on testing set\n",
    "    m=np.shape(testing)\n",
    "    mse=(1/float(m[0]))*sum(np.square(testing.dot(w)-y_test))\n",
    "    print(\"mse for lambda=\",L, \"is:\",mse)\n",
    "   \n",
    "\n",
    "Ridge_Reg(training, testing, y_test, y_train, L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Ridge Regression for multiple lambdas and report the test MSE for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mse for lambda=', 0.01, 'is:', matrix([[ 0.00915107]]))\n",
      "('mse for lambda=', 0.1, 'is:', matrix([[ 0.00915371]]))\n",
      "('mse for lambda=', 1, 'is:', matrix([[ 0.00917912]]))\n",
      "('mse for lambda=', 10, 'is:', matrix([[ 0.00943142]]))\n",
      "('mse for lambda=', 100, 'is:', matrix([[ 0.01097495]]))\n"
     ]
    }
   ],
   "source": [
    "L_vec=[0.01,0.1,1,10,100]\n",
    "\n",
    "for L in L_vec:\n",
    "    Ridge_Reg(training, testing, y_test, y_train, L)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Write the gradient descent with fixed step size for solving (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def StepWise_G(theta_hat,training, y_train, step_size, e):\n",
    "    L=1\n",
    "    i=1\n",
    "    r0=np.linalg.norm(theta_hat)\n",
    "    while i <= 50:\n",
    "        g=np.transpose(training).dot(training.dot(theta_hat)-y_train)+L*theta_hat\n",
    "        g_norm=np.linalg.norm(g)\n",
    "        mse=(1/float(n[0]))*sum(np.square(training.dot(theta_hat)-y_train))\n",
    "        if g_norm <= e*r0:\n",
    "            print(g_norm)\n",
    "        else:\n",
    "            theta_hat=theta_hat-step_size*g\n",
    "        #print(\"Theta was\",g_norm,\" in the ith iteration=\",i)\n",
    "        i=i+1\n",
    "    print(\"MSE was\", mse,\" for step=\",step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE was', matrix([[ 61.18783912]]), ' for step=', 1e-07)\n",
      "('MSE was', matrix([[ 0.4983926]]), ' for step=', 1e-06)\n",
      "('MSE was', matrix([[ 0.0541898]]), ' for step=', 1e-05)\n",
      "('MSE was', matrix([[  2.09323872e+67]]), ' for step=', 0.0001)\n",
      "('MSE was', matrix([[  7.41194388e+172]]), ' for step=', 0.001)\n",
      "('MSE was', matrix([[  3.59849418e+271]]), ' for step=', 0.01)\n"
     ]
    }
   ],
   "source": [
    "step_vec=[1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "theta_hat= np.matrix(np.ones((n[1],1))) \n",
    "for step in step_vec:\n",
    "    StepWise_G(theta_hat,training,y_train,step,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "train_E2006, yE2006_train=Read_Data_SVM(\"E2006.train\")\n",
    "test_E2006, yE2006_test=Read_Data_SVM(\"E2006.test\")\n",
    "train_E2006=sparse.lil_matrix(train_E2006[:,0:150358]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "step_vec=[1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "n=train_E2006.shape\n",
    "theta_hat= np.matrix(np.ones((n[1],1))) \n",
    "for step in step_vec:\n",
    "    StepWise_G(theta_hat,train_E2006,yE2006_train,step,0.001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Classification (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD_Log(theta_hat,training, y_train, step_size, e):\n",
    "    L=1\n",
    "    i=1\n",
    "    r0=np.linalg.norm(theta_hat)\n",
    "    while i <= 50:\n",
    "        g=np.transpose(training).dot(training.dot(theta_hat)-y_train)+L*theta_hat\n",
    "        g_norm=np.linalg.norm(g)\n",
    "        mse=(1/float(n[0]))*sum(np.square(training.dot(theta_hat)-y_train))\n",
    "        if g_norm <= e*r0:\n",
    "            print(g_norm)\n",
    "        else:\n",
    "            theta_hat=theta_hat-step_size*g\n",
    "        #print(\"Theta was\",g_norm,\" in the ith iteration=\",i)\n",
    "        i=i+1\n",
    "    print(\"MSE was\", mse,\" for step=\",step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_X, news_y=Read_Data_SVM('news20.binary')\n",
    "X_train, X_test, y_train, y_test = train_test_split(news_X, news_y, test_size=0.2)\n",
    "m=X_train.shape\n",
    "theta_hat=np.random.rand(m[1],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d81751170f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.transpose(y_train).dot(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Ridge Regression with Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named linar_model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-7c1ba05a8f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinar_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGDRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mridge_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cholesky\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mridge_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0myr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mridge_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named linar_model"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linar_model import SGDRegressor \n",
    "ridge_reg=Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(training, y_train)\n",
    "yr=ridge_reg.predict(testing)\n",
    "print (sklearn.metrics.mean_squared_error(y_test,yr)) #0.00879960756317\n",
    "\n",
    "#Using Stochastic Gradient Descent\n",
    "sgd_reg=SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(training, y_train.ravel())\n",
    "yr_SGD=sgd_reg.predict(testing)\n",
    "print(sklearn.metrics.mean_squared_error(y_test,yr_SGD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
